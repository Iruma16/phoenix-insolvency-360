from __future__ import annotations

from dataclasses import dataclass
from datetime import date
from typing import Literal, Optional

from openai import OpenAI
from sqlalchemy.orm import Session

from app.core.variables import (
    EMBEDDING_MODEL,
    LEGAL_QUALITY_SCORE_BLOCK_THRESHOLD,
    LEGAL_QUALITY_SCORE_WARNING_THRESHOLD,
    RAG_AUTO_BUILD_EMBEDDINGS,
    RAG_HALLUCINATION_RISK_THRESHOLD,
    RAG_MIN_CHUNKS_REQUIRED,
    RAG_MIN_SIMILARITY_SCORE,
    RAG_TRACE_DECISIONS,
    RAG_WEAK_RESPONSE_MAX_DISTANCE,
)
from app.models.document import Document
from app.models.document_chunk import DocumentChunk
from app.rag.evidence import (
    NoResponseReasonCode,
    RetrievalEvidence,
    apply_evidence_gates,
    build_retrieval_evidence,
    log_evidence_decision,
)
from app.rag.evidence_enforcer import (
    validate_chunk_has_location,
    validate_chunks_exist,
    validate_retrieval_result,
)
from app.services.document_chunk_pipeline import (
    build_document_chunks_for_case,
)
from app.services.document_quality import get_document_quality_summary
from app.services.embeddings_pipeline import (
    build_embeddings_for_case,
    get_case_collection,
)
from app.services.vectorstore_versioning import (
    get_active_version,
)

# =========================================================
# ESTADOS INTERNOS RAG
# =========================================================

RAGStatus = Literal[
    "OK",
    "CASE_NOT_FOUND",
    "NO_CHUNKS",
    "NO_EMBEDDINGS",
    "NO_RELEVANT_CONTEXT",
    "PARTIAL_CONTEXT",
]

ConfidenceLevel = Literal["alta", "media", "baja"]


@dataclass
class RAGInternalResult:
    status: RAGStatus
    context_text: str  # Contexto crudo concatenado de chunks
    sources: list[dict]  # Lista de chunks con metadatos y scores
    confidence: ConfidenceLevel
    warnings: list[str]
    hallucination_risk: bool = False  # True si hay alto riesgo de alucinaci√≥n
    evidence: Optional[RetrievalEvidence] = None  # Evidencia verificable (Endurecimiento #4)
    no_response_reason: Optional[NoResponseReasonCode] = None  # C√≥digo si NO_RESPONSE


# =========================================================
# FUNCI√ìN RAG (CEREBRO √öNICO)
# =========================================================


def rag_answer_internal(
    *,
    db: Session,
    case_id: str,
    question: str,
    top_k: int,
    doc_types: Optional[list[str]] = None,
    date_from: Optional[date] = None,
    date_to: Optional[date] = None,
) -> RAGInternalResult:
    warnings: list[str] = []

    # --------------------------------------------------
    # 0Ô∏è‚É£ EVALUAR CALIDAD DOCUMENTAL Y RIESGO LEGAL
    # --------------------------------------------------
    quality_summary = get_document_quality_summary(db=db, case_id=case_id)
    quality_score = quality_summary["quality_score"]
    legal_risks = quality_summary.get("legal_risks", [])
    critical_docs_missing = quality_summary.get("critical_documents_missing", 0)

    # ‚úÖ BLOQUEO: Si la calidad es muy baja, NO permitir conclusiones autom√°ticas
    if quality_score < LEGAL_QUALITY_SCORE_BLOCK_THRESHOLD:
        return RAGInternalResult(
            status="CASE_NOT_FOUND",  # Reutilizamos status para bloqueo
            context_text="",  # Sin contexto disponible por bloqueo
            sources=[],
            confidence="baja",
            warnings=[
                f"‚ö†Ô∏è BLOQUEO POR CALIDAD: Score {quality_score:.1f}/100 (umbral m√≠nimo: {LEGAL_QUALITY_SCORE_BLOCK_THRESHOLD})",
                "Se requieren conclusiones manuales hasta que la documentaci√≥n est√© completamente procesada.",
            ]
            + legal_risks,
            hallucination_risk=True,  # Baja calidad = alto riesgo de alucinaci√≥n
        )

    # ‚úÖ ALERTA: Documentos cr√≠ticos sin embeddings
    if critical_docs_missing > 0:
        warnings.extend(legal_risks)
        warnings.append(
            f"‚ö†Ô∏è ALERTA LEGAL: {critical_docs_missing} documento(s) legal(es) cr√≠tico(s) "
            "sin procesamiento completo. Las conclusiones pueden estar incompletas."
        )

    # ‚úÖ ADVERTENCIA: Calidad baja pero no bloqueante
    if quality_score < LEGAL_QUALITY_SCORE_WARNING_THRESHOLD:
        warnings.append(
            f"‚ö†Ô∏è Calidad documental baja ({quality_score:.1f}/100). "
            "Riesgo elevado de alucinaci√≥n. Se recomienda revisar conclusiones manualmente."
        )

    # ‚úÖ Baja calidad ‚Üí Alta probabilidad de alucinaci√≥n
    # Ajustar umbral de riesgo de alucinaci√≥n basado en calidad
    quality_adjusted_hallucination_threshold = RAG_HALLUCINATION_RISK_THRESHOLD
    if quality_score < 75:
        # Si calidad < 75, ser m√°s estricto con riesgo de alucinaci√≥n
        quality_adjusted_hallucination_threshold = RAG_HALLUCINATION_RISK_THRESHOLD * 0.85

    # --------------------------------------------------
    # 1Ô∏è‚É£ Validar caso + documentos
    # --------------------------------------------------
    from sqlalchemy import or_

    query = db.query(Document).filter(
        Document.case_id == case_id,
        # FASE 2A: Excluir documentos duplicados marcados para exclusi√≥n
        or_(
            Document.duplicate_action.is_(None),
            Document.duplicate_action == "pending",
            Document.duplicate_action == "keep_both",
        ),
    )

    if doc_types:
        query = query.filter(Document.doc_type.in_(doc_types))

    if date_from:
        query = query.filter(Document.date_end >= date_from)

    if date_to:
        query = query.filter(Document.date_start <= date_to)

    documents = query.all()

    if not documents:
        return RAGInternalResult(
            status="CASE_NOT_FOUND",
            context_text="",  # Sin contexto disponible
            sources=[],
            confidence="baja",
            warnings=[],
            hallucination_risk=False,
        )

    document_ids = [d.document_id for d in documents]

    # --------------------------------------------------
    # 2Ô∏è‚É£ Validar / generar chunks (PASO 2)
    # --------------------------------------------------
    # FASE 4: Validaci√≥n DURA - no chunks ‚Üí excepci√≥n
    try:
        chunks_count = validate_chunks_exist(db, case_id, document_ids)
    except Exception:
        # Si falla validaci√≥n, intentar generar chunks autom√°ticamente
        chunks_count = 0

    if chunks_count == 0:
        warnings.append("No hab√≠a chunks y se ha ejecutado el chunking autom√°ticamente.")

        build_document_chunks_for_case(
            db=db,
            case_id=case_id,
            overwrite=False,
        )

        chunks_count = (
            db.query(DocumentChunk)
            .filter(
                DocumentChunk.case_id == case_id,
                DocumentChunk.document_id.in_(document_ids),
            )
            .count()
        )

        if chunks_count == 0:
            return RAGInternalResult(
                status="NO_CHUNKS",
                context_text="",  # Sin chunks disponibles
                sources=[],
                confidence="baja",
                warnings=warnings,
                hallucination_risk=False,
            )

    # --------------------------------------------------
    # 3Ô∏è‚É£ Validar / generar embeddings (PASO 3)
    # --------------------------------------------------
    # CAMBIO: Ahora verificamos que existe una versi√≥n ACTIVE v√°lida
    active_version = get_active_version(case_id)

    if not active_version:
        # No existe versi√≥n activa
        if not RAG_AUTO_BUILD_EMBEDDINGS:
            return RAGInternalResult(
                status="NO_EMBEDDINGS",
                context_text="",  # Sin embeddings disponibles
                sources=[],
                confidence="baja",
                warnings=warnings,
                hallucination_risk=False,
            )

        # Generar embeddings autom√°ticamente
        warnings.append("El √≠ndice sem√°ntico no exist√≠a y se ha generado autom√°ticamente.")
        try:
            version_id = build_embeddings_for_case(db=db, case_id=case_id)
            warnings.append(f"Nueva versi√≥n creada: {version_id}")
        except Exception as e:
            return RAGInternalResult(
                status="NO_EMBEDDINGS",
                context_text="",
                sources=[],
                confidence="baja",
                warnings=warnings + [f"Error generando embeddings: {e}"],
                hallucination_risk=False,
            )

    # Obtener colecci√≥n de la versi√≥n activa (o None para usar ACTIVE)
    try:
        collection = get_case_collection(case_id, version=None)  # None = usar ACTIVE

        # Validar que la colecci√≥n no est√© vac√≠a
        if collection.count() == 0:
            warnings.append("La versi√≥n activa del vectorstore est√° vac√≠a.")
            if not RAG_AUTO_BUILD_EMBEDDINGS:
                return RAGInternalResult(
                    status="NO_EMBEDDINGS",
                    context_text="",
                    sources=[],
                    confidence="baja",
                    warnings=warnings,
                    hallucination_risk=False,
                )

            # Regenerar embeddings
            warnings.append("Regenerando embeddings...")
            try:
                version_id = build_embeddings_for_case(db=db, case_id=case_id)
                warnings.append(f"Nueva versi√≥n creada: {version_id}")
                collection = get_case_collection(case_id, version=None)
            except Exception as e:
                return RAGInternalResult(
                    status="NO_EMBEDDINGS",
                    context_text="",
                    sources=[],
                    confidence="baja",
                    warnings=warnings + [f"Error regenerando embeddings: {e}"],
                    hallucination_risk=False,
                )
    except Exception as e:
        return RAGInternalResult(
            status="NO_EMBEDDINGS",
            context_text="",
            sources=[],
            confidence="baja",
            warnings=warnings + [f"Error accediendo al vectorstore: {e}"],
            hallucination_risk=False,
        )

    # --------------------------------------------------
    # 4Ô∏è‚É£ B√∫squeda sem√°ntica
    # --------------------------------------------------
    # Generar embedding de la pregunta usando el mismo modelo que los embeddings almacenados
    openai_client = OpenAI()
    question_embedding = (
        openai_client.embeddings.create(
            model=EMBEDDING_MODEL,
            input=[question],
        )
        .data[0]
        .embedding
    )

    results = collection.query(
        query_embeddings=[question_embedding],
        n_results=top_k,
        include=["metadatas", "documents", "distances"],  # ‚úÖ Incluir distancias
    )

    docs_found = results.get("documents", [[]])[0]
    metas = results.get("metadatas", [[]])[0]
    distances = results.get("distances", [[]])[0]  # ‚úÖ Obtener distancias (menor = m√°s similar)

    if not docs_found:
        return RAGInternalResult(
            status="NO_RELEVANT_CONTEXT",
            context_text="",  # Sin contexto relevante encontrado
            sources=[],
            confidence="baja",
            warnings=warnings,
            hallucination_risk=False,
        )

    # --------------------------------------------------
    # 5Ô∏è‚É£ Construcci√≥n de contexto + fuentes (CON FILTRO DE SCORE)
    # --------------------------------------------------
    context_blocks = []
    sources = []

    # Filtrar documentos v√°lidos y por score m√≠nimo de similitud
    valid_pairs = []
    for text, meta, distance in zip(docs_found, metas, distances):
        try:
            # ‚úÖ FILTRAR POR SCORE M√çNIMO DE SIMILARIDAD
            if distance > RAG_MIN_SIMILARITY_SCORE:
                continue  # Saltar si la distancia es mayor al umbral

            # Validar texto
            if text is None or not text.strip():
                continue

            # Validar metadatos
            if meta is None:
                continue

            document_id = meta.get("document_id")
            chunk_index = meta.get("chunk_index")

            if document_id is None or chunk_index is None:
                continue

            # Convertir tipos si es necesario (ChromaDB puede devolver strings)
            document_id = str(document_id)
            try:
                chunk_index = int(chunk_index)
            except (ValueError, TypeError):
                continue  # Si no se puede convertir a int, saltar este elemento

            # ‚úÖ Incluir distancia en los datos
            valid_pairs.append(
                (text, {"document_id": document_id, "chunk_index": chunk_index}, distance)
            )
        except Exception as e:
            # Si hay alg√∫n error procesando este elemento, saltarlo
            print(f"[WARN] Error procesando elemento en validaci√≥n: {e}")
            continue

    # REGLA 4: Umbral m√≠nimo de contexto (BLOQUEANTE)
    if len(valid_pairs) < RAG_MIN_CHUNKS_REQUIRED:
        if RAG_TRACE_DECISIONS:
            print(
                f"[RAG DECISI√ìN] ‚ùå BLOQUEO: {len(valid_pairs)} chunks < {RAG_MIN_CHUNKS_REQUIRED} m√≠nimo requerido"
            )
            print("[RAG DECISI√ìN] Motivo: EVIDENCIA_INSUFICIENTE")

        return RAGInternalResult(
            status="NO_RELEVANT_CONTEXT",
            context_text="",  # Sin contexto v√°lido encontrado
            sources=[],
            confidence="baja",
            warnings=warnings
            + [
                f"Se encontraron {len(valid_pairs)} fragmento(s) relevante(s), "
                f"pero se requieren al menos {RAG_MIN_CHUNKS_REQUIRED} para generar una respuesta fundamentada."
            ],
            hallucination_risk=False,
        )

    # ‚úÖ Determinar riesgo de alucinaci√≥n y respuesta d√©bil basado en la mejor distancia
    # Usar umbral ajustado por calidad documental (baja calidad ‚Üí m√°s estricto)
    best_distance = valid_pairs[0][2] if valid_pairs else float("inf")
    hallucination_risk = best_distance > quality_adjusted_hallucination_threshold
    is_weak_response = best_distance > RAG_WEAK_RESPONSE_MAX_DISTANCE

    # ‚úÖ Baja calidad documental aumenta riesgo de alucinaci√≥n
    if quality_score < LEGAL_QUALITY_SCORE_WARNING_THRESHOLD:
        # Si calidad es baja, marcar como riesgo de alucinaci√≥n incluso con mejor similitud
        if not hallucination_risk and quality_score < 65:
            hallucination_risk = True
            warnings.append(
                "Riesgo de alucinaci√≥n elevado debido a baja calidad documental del caso."
            )

    # ‚úÖ A√±adir warnings orientados a valor legal (no t√©cnico)
    if is_weak_response:
        warnings.append(
            "‚ö†Ô∏è La informaci√≥n encontrada tiene baja relevancia para la consulta. "
            "Se recomienda revisar manualmente la respuesta y contrastarla con la documentaci√≥n original."
        )

    if hallucination_risk:
        warnings.append(
            f"‚ö†Ô∏è RIESGO LEGAL: Alto riesgo de que la respuesta contenga informaci√≥n incorrecta o inventada. "
            f"Calidad documental: {quality_score:.1f}/100. "
            "Se requiere verificaci√≥n exhaustiva con la documentaci√≥n original antes de usar esta respuesta."
        )

    # REGLA 3: Evidencia obligatoria - enriquecer sources con metadata completa
    for text, meta, distance in valid_pairs:
        # Los tipos ya est√°n validados y convertidos en el paso anterior
        document_id = meta["document_id"]
        chunk_index = meta["chunk_index"]

        # Obtener chunk desde BD para metadata completa (chunk_id, page, offsets)
        # ‚úÖ SEGURIDAD: Filtrar por case_id para prevenir acceso cross-case
        chunk = (
            db.query(DocumentChunk)
            .filter(
                DocumentChunk.case_id == case_id,  # ‚úÖ CR√çTICO: Aislamiento por expediente
                DocumentChunk.document_id == document_id,
                DocumentChunk.chunk_index == chunk_index,
            )
            .first()
        )

        # ‚úÖ SEGURIDAD: Si el chunk no pertenece a este case_id, skipear
        if not chunk:
            continue  # Chunk no encontrado o no pertenece a este caso

        # FASE 4: Validar que el chunk tiene location obligatoria
        try:
            validate_chunk_has_location(chunk)
        except Exception as e:
            # Si el chunk no tiene location v√°lida, skipear
            if RAG_TRACE_DECISIONS:
                print(f"[RAG] ‚ö†Ô∏è Chunk sin location v√°lida skipeado: {e}")
            continue

        # Construir fuente con TODA la metadata necesaria para citaci√≥n
        source = {
            "document_id": document_id,
            "chunk_index": chunk_index,
            "content": text,
            "similarity_score": round(distance, 4),
        }

        if chunk:
            # REGLA 3: Metadata obligatoria para citaci√≥n precisa
            source["chunk_id"] = chunk.chunk_id
            source["page"] = chunk.page
            source["start_char"] = chunk.start_char
            source["end_char"] = chunk.end_char
            source["section_hint"] = chunk.section_hint

            # Obtener filename para citaci√≥n
            if chunk.document:
                source["filename"] = chunk.document.filename

        context_blocks.append(f"[Documento {document_id} | Chunk {chunk_index}]\n{text}")
        sources.append(source)

    context = "\n\n".join(context_blocks)

    # --------------------------------------------------
    # üîí FASE 4: VALIDAR EVIDENCIA ANTES DE CONTINUAR
    # --------------------------------------------------
    # FAIL HARD si no hay evidencia suficiente
    try:
        validate_retrieval_result(sources, case_id)
    except Exception as validation_error:
        if RAG_TRACE_DECISIONS:
            print(f"[RAG] ‚ùå Validaci√≥n de evidencia fall√≥: {validation_error}")

        # Retornar NO_RELEVANT_CONTEXT con informaci√≥n del error
        return RAGInternalResult(
            status="NO_RELEVANT_CONTEXT",
            context_text="",
            sources=[],
            confidence="baja",
            warnings=warnings + [str(validation_error)],
            hallucination_risk=False,
            evidence=None,
            no_response_reason=NoResponseReasonCode.EVIDENCE_INSUFFICIENT,
        )

    # --------------------------------------------------
    # üîí ENDURECIMIENTO #4: VALIDAR EVIDENCIA (FAIL HARD)
    # --------------------------------------------------
    evidence = build_retrieval_evidence(sources)
    reason_code = apply_evidence_gates(evidence)

    # Logging obligatorio
    log_evidence_decision(
        case_id=case_id,
        evidence=evidence,
        reason_code=reason_code,
    )

    # GATE BLOQUEANTE: Si evidencia insuficiente ‚Üí NO_RESPONSE
    if reason_code:
        if RAG_TRACE_DECISIONS:
            print(f"[RAG DECISI√ìN] ‚ùå NO_RESPONSE: {reason_code.value}")
            print(f"[RAG DECISI√ìN] Evidence stats: {evidence.get_stats()}")

        return RAGInternalResult(
            status="NO_RELEVANT_CONTEXT",
            context_text="",
            sources=[],
            confidence="baja",
            warnings=warnings
            + [
                f"NO_RESPONSE: {reason_code.value}",
                f"Valid chunks: {evidence.valid_chunks}/{evidence.total_chunks}",
                f"Avg similarity: {evidence.avg_similarity:.4f}",
            ],
            hallucination_risk=False,
            evidence=evidence,
            no_response_reason=reason_code,
        )

    # ‚úÖ Determinar confianza basada en cantidad, calidad de similitud Y calidad documental
    if len(valid_pairs) < top_k:
        warnings.append(
            "‚ö†Ô∏è La respuesta se basa en informaci√≥n parcial de la documentaci√≥n. "
            "Puede omitir informaci√≥n relevante."
        )
        confidence: ConfidenceLevel = "media"
        status: RAGStatus = "PARTIAL_CONTEXT"
    else:
        # Si tenemos suficientes resultados, la confianza depende de:
        # 1. Calidad de similitud (distancia)
        # 2. Calidad documental del caso (quality_score)

        # Baja calidad documental reduce confianza
        if quality_score < 65:
            confidence = "baja"  # Calidad muy baja = baja confianza
            status = "OK"
        elif hallucination_risk or quality_score < LEGAL_QUALITY_SCORE_WARNING_THRESHOLD:
            confidence = "baja"  # Alto riesgo de alucinaci√≥n o calidad baja = baja confianza
            status = "OK"
        elif is_weak_response:
            confidence = "media"  # Respuesta d√©bil = confianza media
            status = "OK"
        else:
            confidence = "alta"  # Buena similitud y calidad = alta confianza
            status = "OK"

    # --------------------------------------------------
    # 6Ô∏è‚É£ Devolver contexto crudo (sin LLM)
    # --------------------------------------------------
    # RAG solo recupera contexto, NO genera respuestas

    # REGLA 6: Decisi√≥n trazable
    if RAG_TRACE_DECISIONS:
        print(f"\n{'='*80}")
        print(f"[RAG DECISI√ìN] case_id={case_id}")
        print("[RAG DECISI√ìN] ‚úÖ RESPUESTA_CON_EVIDENCIA")
        print(f"[RAG DECISI√ìN] Chunks recuperados: {len(sources)}")
        print(f"[RAG DECISI√ìN] Confianza: {confidence}")
        print(f"[RAG DECISI√ìN] Riesgo alucinaci√≥n: {'S√ç' if hallucination_risk else 'NO'}")
        print(f"[RAG DECISI√ìN] Calidad documental: {quality_score:.1f}/100")
        print("[RAG DECISI√ìN] Chunks usados:")
        for i, src in enumerate(sources, 1):
            print(
                f"  {i}. chunk_id={src.get('chunk_id', 'N/A')[:16]}... | "
                f"doc={src['document_id'][:8]}... | "
                f"score={src['similarity_score']:.3f} | "
                f"page={src.get('page', 'N/A')}"
            )
        print(f"{'='*80}\n")

    return RAGInternalResult(
        status=status,
        context_text=context,  # Contexto crudo concatenado
        sources=sources,
        confidence=confidence,
        warnings=warnings,
        hallucination_risk=hallucination_risk,  # ‚úÖ Incluir flag de riesgo de alucinaci√≥n
        evidence=evidence,  # ‚úÖ Evidencia verificable
        no_response_reason=None,  # Sin bloqueo, evidencia v√°lida
    )
